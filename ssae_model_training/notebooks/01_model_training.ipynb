{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Student Success Analytics & Early Intervention System (SSAES)\n",
    "## Model Training Pipeline\n",
    "\n",
    "This notebook implements a complete machine learning pipeline for predicting student performance and identifying at-risk students.\n",
    "\n",
    "**Dataset Location:** `data/demo/your_dataset.csv`\n",
    "\n",
    "**Installation:** Run `pip install -r requirements.txt` before executing this notebook.\n",
    "\n",
    "**Outputs:**\n",
    "- Trained models saved to `models/`\n",
    "- Evaluation plots saved to `reports/figures/`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Setup & Imports\n",
    "\n",
    "Import all necessary libraries and set up the environment for reproducible results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# Machine Learning\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, roc_curve\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import xgboost as xgb\n",
    "import joblib\n",
    "import shap\n",
    "\n",
    "# Add src to path for utils\n",
    "sys.path.append('../src')\n",
    "from utils import load_data, save_model, load_model, plot_confusion_matrix\n",
    "\n",
    "# Settings\n",
    "np.random.seed(42)\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)\n",
    "\n",
    "print(\"‚úÖ All libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load Dataset\n",
    "\n",
    "Load the student performance dataset and perform initial exploration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "data_path = '../data/demo/your_dataset.csv'\n",
    "df = load_data(data_path)\n",
    "\n",
    "if df is not None:\n",
    "    print(\"\\nüìä Dataset Overview:\")\n",
    "    print(f\"Shape: {df.shape}\")\n",
    "    print(\"\\nüîç First 5 rows:\")\n",
    "    display(df.head())\n",
    "    \n",
    "    print(\"\\nüìã Dataset Info:\")\n",
    "    df.info()\n",
    "    \n",
    "    print(\"\\nüìà Statistical Summary:\")\n",
    "    display(df.describe())\n",
    "    \n",
    "    # Save sample data\n",
    "    os.makedirs('../reports/figures', exist_ok=True)\n",
    "    df.head(10).to_csv('../reports/figures/sample_data.csv', index=False)\n",
    "    print(\"\\n‚úÖ Sample data saved to reports/figures/sample_data.csv\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è  Please upload your dataset to data/demo/ folder and update the filename above.\")\n",
    "    print(\"Expected columns: student_id, final_marks, pass_fail, attendance_rate, etc.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Quick Data Quality Report\n",
    "\n",
    "Analyze data quality issues including missing values, duplicates, and class balance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if df is not None:\n",
    "    print(\"üîç Data Quality Assessment\\n\")\n",
    "    \n",
    "    # Missing values analysis\n",
    "    missing_data = df.isnull().sum()\n",
    "    missing_percent = (missing_data / len(df)) * 100\n",
    "    missing_df = pd.DataFrame({\n",
    "        'Missing Count': missing_data,\n",
    "        'Missing Percentage': missing_percent\n",
    "    }).sort_values('Missing Count', ascending=False)\n",
    "    \n",
    "    print(\"üìä Missing Values Summary:\")\n",
    "    display(missing_df[missing_df['Missing Count'] > 0])\n",
    "    \n",
    "    # Duplicates check\n",
    "    duplicates = df.duplicated().sum()\n",
    "    print(f\"\\nüîÑ Duplicate rows: {duplicates}\")\n",
    "    \n",
    "    # Data types\n",
    "    print(\"\\nüìã Data Types:\")\n",
    "    print(df.dtypes.value_counts())\n",
    "    \n",
    "    # Visualize missing data\n",
    "    if missing_data.sum() > 0:\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        sns.heatmap(df.isnull(), cbar=True, yticklabels=False, cmap='viridis')\n",
    "        plt.title('Missing Data Heatmap')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig('../reports/figures/missing_data_heatmap.png', dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "    \n",
    "    # Class balance for pass_fail (if exists)\n",
    "    if 'pass_fail' in df.columns:\n",
    "        plt.figure(figsize=(8, 5))\n",
    "        df['pass_fail'].value_counts().plot(kind='bar')\n",
    "        plt.title('Class Distribution: Pass/Fail')\n",
    "        plt.xlabel('Outcome')\n",
    "        plt.ylabel('Count')\n",
    "        plt.xticks(rotation=0)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig('../reports/figures/class_balance.png', dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "        \n",
    "        print(f\"\\nüìä Class Balance:\")\n",
    "        print(df['pass_fail'].value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Preprocessing & Feature Engineering\n",
    "\n",
    "Clean the data and create features suitable for machine learning models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if df is not None:\n",
    "    print(\"üîß Data Preprocessing & Feature Engineering\\n\")\n",
    "    \n",
    "    # Create a copy for processing\n",
    "    df_processed = df.copy()\n",
    "    \n",
    "    print(f\"Original shape: {df_processed.shape}\")\n",
    "    \n",
    "    # Handle missing values\n",
    "    numeric_cols = df_processed.select_dtypes(include=[np.number]).columns\n",
    "    categorical_cols = df_processed.select_dtypes(include=['object']).columns\n",
    "    \n",
    "    # Fill numeric missing values with median\n",
    "    for col in numeric_cols:\n",
    "        if df_processed[col].isnull().sum() > 0:\n",
    "            df_processed[col].fillna(df_processed[col].median(), inplace=True)\n",
    "            print(f\"‚úÖ Filled {col} missing values with median\")\n",
    "    \n",
    "    # Fill categorical missing values with mode\n",
    "    for col in categorical_cols:\n",
    "        if df_processed[col].isnull().sum() > 0:\n",
    "            df_processed[col].fillna(df_processed[col].mode()[0], inplace=True)\n",
    "            print(f\"‚úÖ Filled {col} missing values with mode\")\n",
    "    \n",
    "    # Remove duplicates\n",
    "    df_processed.drop_duplicates(inplace=True)\n",
    "    print(f\"‚úÖ Removed duplicates. New shape: {df_processed.shape}\")\n",
    "    \n",
    "    # Feature Engineering Examples\n",
    "    \n",
    "    # Create engagement_index if not present\n",
    "    if 'engagement_index' not in df_processed.columns and 'attendance_rate' in df_processed.columns:\n",
    "        # Simple engagement index based on attendance\n",
    "        df_processed['engagement_index'] = df_processed['attendance_rate'] * 0.7 + np.random.normal(0.3, 0.1, len(df_processed))\n",
    "        df_processed['engagement_index'] = np.clip(df_processed['engagement_index'], 0, 1)\n",
    "        print(\"‚úÖ Created engagement_index feature\")\n",
    "    \n",
    "    # Create attendance trend (if multiple attendance columns exist)\n",
    "    attendance_cols = [col for col in df_processed.columns if 'attendance' in col.lower()]\n",
    "    if len(attendance_cols) > 1:\n",
    "        df_processed['attendance_trend'] = df_processed[attendance_cols].mean(axis=1)\n",
    "        print(\"‚úÖ Created attendance_trend feature\")\n",
    "    \n",
    "    # Encode categorical variables\n",
    "    label_encoders = {}\n",
    "    categorical_cols = df_processed.select_dtypes(include=['object']).columns\n",
    "    \n",
    "    for col in categorical_cols:\n",
    "        if col not in ['student_id']:  # Don't encode ID columns\n",
    "            le = LabelEncoder()\n",
    "            df_processed[col] = le.fit_transform(df_processed[col])\n",
    "            label_encoders[col] = le\n",
    "            print(f\"‚úÖ Label encoded {col}\")\n",
    "    \n",
    "    print(f\"\\nüìä Processed dataset shape: {df_processed.shape}\")\n",
    "    print(\"\\nüìã Final data types:\")\n",
    "    print(df_processed.dtypes.value_counts())\n",
    "    \n",
    "    # Display processed data sample\n",
    "    print(\"\\nüîç Processed data sample:\")\n",
    "    display(df_processed.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Train/Test Split\n",
    "\n",
    "Split the data into training and testing sets with stratification for classification tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if df is not None:\n",
    "    print(\"üîÑ Creating Train/Test Split\\n\")\n",
    "    \n",
    "    # Define features and targets\n",
    "    feature_cols = [col for col in df_processed.columns if col not in ['student_id', 'final_marks', 'pass_fail']]\n",
    "    X = df_processed[feature_cols]\n",
    "    \n",
    "    # Regression target\n",
    "    if 'final_marks' in df_processed.columns:\n",
    "        y_reg = df_processed['final_marks']\n",
    "        print(f\"‚úÖ Regression target: final_marks (range: {y_reg.min():.1f} - {y_reg.max():.1f})\")\n",
    "    \n",
    "    # Classification target\n",
    "    if 'pass_fail' in df_processed.columns:\n",
    "        y_class = df_processed['pass_fail']\n",
    "        print(f\"‚úÖ Classification target: pass_fail (classes: {y_class.unique()})\")\n",
    "    \n",
    "    print(f\"\\nüìä Features: {len(feature_cols)} columns\")\n",
    "    print(f\"Feature names: {feature_cols}\")\n",
    "    \n",
    "    # Split for regression\n",
    "    if 'final_marks' in df_processed.columns:\n",
    "        X_train_reg, X_test_reg, y_train_reg, y_test_reg = train_test_split(\n",
    "            X, y_reg, test_size=0.2, random_state=42\n",
    "        )\n",
    "        print(f\"\\nüìà Regression split - Train: {X_train_reg.shape[0]}, Test: {X_test_reg.shape[0]}\")\n",
    "    \n",
    "    # Split for classification (with stratification)\n",
    "    if 'pass_fail' in df_processed.columns:\n",
    "        X_train_class, X_test_class, y_train_class, y_test_class = train_test_split(\n",
    "            X, y_class, test_size=0.2, random_state=42, stratify=y_class\n",
    "        )\n",
    "        print(f\"üìä Classification split - Train: {X_train_class.shape[0]}, Test: {X_test_class.shape[0]}\")\n",
    "    \n",
    "    # Scale features\n",
    "    scaler = StandardScaler()\n",
    "    \n",
    "    if 'final_marks' in df_processed.columns:\n",
    "        X_train_reg_scaled = scaler.fit_transform(X_train_reg)\n",
    "        X_test_reg_scaled = scaler.transform(X_test_reg)\n",
    "        print(\"‚úÖ Features scaled for regression\")\n",
    "    \n",
    "    if 'pass_fail' in df_processed.columns:\n",
    "        X_train_class_scaled = scaler.fit_transform(X_train_class)\n",
    "        X_test_class_scaled = scaler.transform(X_test_class)\n",
    "        print(\"‚úÖ Features scaled for classification\")\n",
    "    \n",
    "    # Save splits\n",
    "    if 'pass_fail' in df_processed.columns:\n",
    "        train_df = pd.concat([pd.DataFrame(X_train_class, columns=feature_cols), y_train_class.reset_index(drop=True)], axis=1)\n",
    "        test_df = pd.concat([pd.DataFrame(X_test_class, columns=feature_cols), y_test_class.reset_index(drop=True)], axis=1)\n",
    "        \n",
    "        train_df.to_csv('../data/demo/train.csv', index=False)\n",
    "        test_df.to_csv('../data/demo/test.csv', index=False)\n",
    "        print(\"‚úÖ Train/test splits saved to data/demo/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Baseline Models (Regression & Classification)\n",
    "\n",
    "Train simple baseline models to establish performance benchmarks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if df is not None:\n",
    "    print(\"üéØ Training Baseline Models\\n\")\n",
    "    \n",
    "    # Regression Baseline: Linear Regression\n",
    "    if 'final_marks' in df_processed.columns:\n",
    "        print(\"üìà Regression Baseline: Linear Regression\")\n",
    "        \n",
    "        lr_reg = LinearRegression()\n",
    "        lr_reg.fit(X_train_reg_scaled, y_train_reg)\n",
    "        \n",
    "        # Predictions\n",
    "        y_pred_reg = lr_reg.predict(X_test_reg_scaled)\n",
    "        \n",
    "        # Metrics\n",
    "        rmse = np.sqrt(mean_squared_error(y_test_reg, y_pred_reg))\n",
    "        mae = mean_absolute_error(y_test_reg, y_pred_reg)\n",
    "        r2 = r2_score(y_test_reg, y_pred_reg)\n",
    "        \n",
    "        print(f\"  RMSE: {rmse:.3f}\")\n",
    "        print(f\"  MAE: {mae:.3f}\")\n",
    "        print(f\"  R¬≤: {r2:.3f}\")\n",
    "        \n",
    "        # Plot residuals\n",
    "        plt.figure(figsize=(10, 4))\n",
    "        \n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.scatter(y_test_reg, y_pred_reg, alpha=0.6)\n",
    "        plt.plot([y_test_reg.min(), y_test_reg.max()], [y_test_reg.min(), y_test_reg.max()], 'r--')\n",
    "        plt.xlabel('Actual')\n",
    "        plt.ylabel('Predicted')\n",
    "        plt.title('Actual vs Predicted')\n",
    "        \n",
    "        plt.subplot(1, 2, 2)\n",
    "        residuals = y_test_reg - y_pred_reg\n",
    "        plt.scatter(y_pred_reg, residuals, alpha=0.6)\n",
    "        plt.axhline(y=0, color='r', linestyle='--')\n",
    "        plt.xlabel('Predicted')\n",
    "        plt.ylabel('Residuals')\n",
    "        plt.title('Residual Plot')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig('../reports/figures/regression_baseline.png', dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "    \n",
    "    # Classification Baseline: Logistic Regression\n",
    "    if 'pass_fail' in df_processed.columns:\n",
    "        print(\"\\nüìä Classification Baseline: Logistic Regression\")\n",
    "        \n",
    "        lr_class = LogisticRegression(random_state=42, max_iter=1000)\n",
    "        lr_class.fit(X_train_class_scaled, y_train_class)\n",
    "        \n",
    "        # Predictions\n",
    "        y_pred_class = lr_class.predict(X_test_class_scaled)\n",
    "        y_pred_proba = lr_class.predict_proba(X_test_class_scaled)[:, 1]\n",
    "        \n",
    "        # Metrics\n",
    "        accuracy = accuracy_score(y_test_class, y_pred_class)\n",
    "        precision = precision_score(y_test_class, y_pred_class, average='weighted')\n",
    "        recall = recall_score(y_test_class, y_pred_class, average='weighted')\n",
    "        f1 = f1_score(y_test_class, y_pred_class, average='weighted')\n",
    "        \n",
    "        print(f\"  Accuracy: {accuracy:.3f}\")\n",
    "        print(f\"  Precision: {precision:.3f}\")\n",
    "        print(f\"  Recall: {recall:.3f}\")\n",
    "        print(f\"  F1-Score: {f1:.3f}\")\n",
    "        \n",
    "        # Confusion Matrix\n",
    "        plot_confusion_matrix(y_test_class, y_pred_class, \n",
    "                            save_path='../reports/figures/confusion_matrix_baseline.png',\n",
    "                            title='Baseline Logistic Regression')\n",
    "        \n",
    "        print(\"\\nüìã Classification Report:\")\n",
    "        print(classification_report(y_test_class, y_pred_class))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Advanced Models & Hyperparameter Tuning\n",
    "\n",
    "Train Random Forest and XGBoost models with hyperparameter optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if df is not None:\n",
    "    print(\"üöÄ Advanced Models & Hyperparameter Tuning\\n\")\n",
    "    \n",
    "    # Store results for comparison\n",
    "    regression_results = {}\n",
    "    classification_results = {}\n",
    "    \n",
    "    # Add baseline results\n",
    "    if 'final_marks' in df_processed.columns:\n",
    "        regression_results['Linear Regression'] = {\n",
    "            'RMSE': rmse, 'MAE': mae, 'R¬≤': r2\n",
    "        }\n",
    "    \n",
    "    if 'pass_fail' in df_processed.columns:\n",
    "        classification_results['Logistic Regression'] = {\n",
    "            'Accuracy': accuracy, 'Precision': precision, 'Recall': recall, 'F1': f1\n",
    "        }\n",
    "    \n",
    "    # Random Forest Regression\n",
    "    if 'final_marks' in df_processed.columns:\n",
    "        print(\"üå≤ Random Forest Regression\")\n",
    "        \n",
    "        rf_reg_params = {\n",
    "            'n_estimators': [50, 100],\n",
    "            'max_depth': [5, 10, None],\n",
    "            'min_samples_split': [2, 5]\n",
    "        }\n",
    "        \n",
    "        rf_reg = RandomForestRegressor(random_state=42)\n",
    "        rf_reg_grid = GridSearchCV(rf_reg, rf_reg_params, cv=3, scoring='neg_mean_squared_error', n_jobs=-1)\n",
    "        rf_reg_grid.fit(X_train_reg, y_train_reg)\n",
    "        \n",
    "        print(f\"  Best params: {rf_reg_grid.best_params_}\")\n",
    "        \n",
    "        # Evaluate\n",
    "        y_pred_rf_reg = rf_reg_grid.predict(X_test_reg)\n",
    "        rmse_rf = np.sqrt(mean_squared_error(y_test_reg, y_pred_rf_reg))\n",
    "        mae_rf = mean_absolute_error(y_test_reg, y_pred_rf_reg)\n",
    "        r2_rf = r2_score(y_test_reg, y_pred_rf_reg)\n",
    "        \n",
    "        regression_results['Random Forest'] = {\n",
    "            'RMSE': rmse_rf, 'MAE': mae_rf, 'R¬≤': r2_rf\n",
    "        }\n",
    "        \n",
    "        print(f\"  RMSE: {rmse_rf:.3f}, MAE: {mae_rf:.3f}, R¬≤: {r2_rf:.3f}\")\n",
    "    \n",
    "    # Random Forest Classification\n",
    "    if 'pass_fail' in df_processed.columns:\n",
    "        print(\"\\nüå≤ Random Forest Classification\")\n",
    "        \n",
    "        rf_class_params = {\n",
    "            'n_estimators': [50, 100],\n",
    "            'max_depth': [5, 10, None],\n",
    "            'min_samples_split': [2, 5]\n",
    "        }\n",
    "        \n",
    "        rf_class = RandomForestClassifier(random_state=42)\n",
    "        rf_class_grid = GridSearchCV(rf_class, rf_class_params, cv=3, scoring='accuracy', n_jobs=-1)\n",
    "        rf_class_grid.fit(X_train_class, y_train_class)\n",
    "        \n",
    "        print(f\"  Best params: {rf_class_grid.best_params_}\")\n",
    "        \n",
    "        # Evaluate\n",
    "        y_pred_rf_class = rf_class_grid.predict(X_test_class)\n",
    "        acc_rf = accuracy_score(y_test_class, y_pred_rf_class)\n",
    "        prec_rf = precision_score(y_test_class, y_pred_rf_class, average='weighted')\n",
    "        rec_rf = recall_score(y_test_class, y_pred_rf_class, average='weighted')\n",
    "        f1_rf = f1_score(y_test_class, y_pred_rf_class, average='weighted')\n",
    "        \n",
    "        classification_results['Random Forest'] = {\n",
    "            'Accuracy': acc_rf, 'Precision': prec_rf, 'Recall': rec_rf, 'F1': f1_rf\n",
    "        }\n",
    "        \n",
    "        print(f\"  Accuracy: {acc_rf:.3f}, Precision: {prec_rf:.3f}, Recall: {rec_rf:.3f}, F1: {f1_rf:.3f}\")\n",
    "    \n",
    "    # XGBoost Regression\n",
    "    if 'final_marks' in df_processed.columns:\n",
    "        print(\"\\n‚ö° XGBoost Regression\")\n",
    "        \n",
    "        xgb_reg_params = {\n",
    "            'n_estimators': [50, 100],\n",
    "            'max_depth': [3, 6],\n",
    "            'learning_rate': [0.1, 0.2]\n",
    "        }\n",
    "        \n",
    "        xgb_reg = xgb.XGBRegressor(random_state=42)\n",
    "        xgb_reg_grid = GridSearchCV(xgb_reg, xgb_reg_params, cv=3, scoring='neg_mean_squared_error', n_jobs=-1)\n",
    "        xgb_reg_grid.fit(X_train_reg, y_train_reg)\n",
    "        \n",
    "        print(f\"  Best params: {xgb_reg_grid.best_params_}\")\n",
    "        \n",
    "        # Evaluate\n",
    "        y_pred_xgb_reg = xgb_reg_grid.predict(X_test_reg)\n",
    "        rmse_xgb = np.sqrt(mean_squared_error(y_test_reg, y_pred_xgb_reg))\n",
    "        mae_xgb = mean_absolute_error(y_test_reg, y_pred_xgb_reg)\n",
    "        r2_xgb = r2_score(y_test_reg, y_pred_xgb_reg)\n",
    "        \n",
    "        regression_results['XGBoost'] = {\n",
    "            'RMSE': rmse_xgb, 'MAE': mae_xgb, 'R¬≤': r2_xgb\n",
    "        }\n",
    "        \n",
    "        print(f\"  RMSE: {rmse_xgb:.3f}, MAE: {mae_xgb:.3f}, R¬≤: {r2_xgb:.3f}\")\n",
    "    \n",
    "    # XGBoost Classification\n",
    "    if 'pass_fail' in df_processed.columns:\n",
    "        print(\"\\n‚ö° XGBoost Classification\")\n",
    "        \n",
    "        xgb_class_params = {\n",
    "            'n_estimators': [50, 100],\n",
    "            'max_depth': [3, 6],\n",
    "            'learning_rate': [0.1, 0.2]\n",
    "        }\n",
    "        \n",
    "        xgb_class = xgb.XGBClassifier(random_state=42)\n",
    "        xgb_class_grid = GridSearchCV(xgb_class, xgb_class_params, cv=3, scoring='accuracy', n_jobs=-1)\n",
    "        xgb_class_grid.fit(X_train_class, y_train_class)\n",
    "        \n",
    "        print(f\"  Best params: {xgb_class_grid.best_params_}\")\n",
    "        \n",
    "        # Evaluate\n",
    "        y_pred_xgb_class = xgb_class_grid.predict(X_test_class)\n",
    "        acc_xgb = accuracy_score(y_test_class, y_pred_xgb_class)\n",
    "        prec_xgb = precision_score(y_test_class, y_pred_xgb_class, average='weighted')\n",
    "        rec_xgb = recall_score(y_test_class, y_pred_xgb_class, average='weighted')\n",
    "        f1_xgb = f1_score(y_test_class, y_pred_xgb_class, average='weighted')\n",
    "        \n",
    "        classification_results['XGBoost'] = {\n",
    "            'Accuracy': acc_xgb, 'Precision': prec_xgb, 'Recall': rec_xgb, 'F1': f1_xgb\n",
    "        }\n",
    "        \n",
    "        print(f\"  Accuracy: {acc_xgb:.3f}, Precision: {prec_xgb:.3f}, Recall: {rec_xgb:.3f}, F1: {f1_xgb:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Model Evaluation & Comparison\n",
    "\n",
    "Compare all models and create comprehensive evaluation reports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if df is not None:\n",
    "    print(\"üìä Model Evaluation & Comparison\\n\")\n",
    "    \n",
    "    # Regression Results Table\n",
    "    if 'final_marks' in df_processed.columns and regression_results:\n",
    "        print(\"üìà Regression Models Comparison:\")\n",
    "        reg_comparison = pd.DataFrame(regression_results).T\n",
    "        reg_comparison = reg_comparison.round(3)\n",
    "        display(reg_comparison)\n",
    "        \n",
    "        # Save comparison\n",
    "        reg_comparison.to_csv('../reports/figures/regression_model_comparison.csv')\n",
    "        print(\"‚úÖ Regression comparison saved\")\n",
    "        \n",
    "        # Find best model\n",
    "        best_reg_model = reg_comparison['R¬≤'].idxmax()\n",
    "        print(f\"\\nüèÜ Best Regression Model: {best_reg_model} (R¬≤ = {reg_comparison.loc[best_reg_model, 'R¬≤']:.3f})\")\n",
    "    \n",
    "    # Classification Results Table\n",
    "    if 'pass_fail' in df_processed.columns and classification_results:\n",
    "        print(\"\\nüìä Classification Models Comparison:\")\n",
    "        class_comparison = pd.DataFrame(classification_results).T\n",
    "        class_comparison = class_comparison.round(3)\n",
    "        display(class_comparison)\n",
    "        \n",
    "        # Save comparison\n",
    "        class_comparison.to_csv('../reports/figures/classification_model_comparison.csv')\n",
    "        print(\"‚úÖ Classification comparison saved\")\n",
    "        \n",
    "        # Find best model\n",
    "        best_class_model = class_comparison['F1'].idxmax()\n",
    "        print(f\"\\nüèÜ Best Classification Model: {best_class_model} (F1 = {class_comparison.loc[best_class_model, 'F1']:.3f})\")\n",
    "        \n",
    "        # ROC Curve for classification\n",
    "        if 'pass_fail' in df_processed.columns:\n",
    "            plt.figure(figsize=(8, 6))\n",
    "            \n",
    "            # Plot ROC for each model\n",
    "            models_to_plot = [\n",
    "                ('Logistic Regression', lr_class, X_test_class_scaled),\n",
    "                ('Random Forest', rf_class_grid.best_estimator_, X_test_class),\n",
    "                ('XGBoost', xgb_class_grid.best_estimator_, X_test_class)\n",
    "            ]\n",
    "            \n",
    "            for name, model, X_test_data in models_to_plot:\n",
    "                try:\n",
    "                    y_proba = model.predict_proba(X_test_data)[:, 1]\n",
    "                    fpr, tpr, _ = roc_curve(y_test_class, y_proba)\n",
    "                    auc_score = roc_auc_score(y_test_class, y_proba)\n",
    "                    plt.plot(fpr, tpr, label=f'{name} (AUC = {auc_score:.3f})')\n",
    "                except:\n",
    "                    pass\n",
    "            \n",
    "            plt.plot([0, 1], [0, 1], 'k--', label='Random')\n",
    "            plt.xlabel('False Positive Rate')\n",
    "            plt.ylabel('True Positive Rate')\n",
    "            plt.title('ROC Curves Comparison')\n",
    "            plt.legend()\n",
    "            plt.grid(True, alpha=0.3)\n",
    "            plt.tight_layout()\n",
    "            plt.savefig('../reports/figures/roc_curves_comparison.png', dpi=300, bbox_inches='tight')\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Model Persistence\n",
    "\n",
    "Save the best performing models for future use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if df is not None:\n",
    "    print(\"üíæ Saving Best Models\\n\")\n",
    "    \n",
    "    # Save best regression model\n",
    "    if 'final_marks' in df_processed.columns:\n",
    "        if best_reg_model == 'Random Forest':\n",
    "            best_regressor = rf_reg_grid.best_estimator_\n",
    "        elif best_reg_model == 'XGBoost':\n",
    "            best_regressor = xgb_reg_grid.best_estimator_\n",
    "        else:\n",
    "            best_regressor = lr_reg\n",
    "        \n",
    "        save_model(best_regressor, '../models/best_regressor.pkl')\n",
    "        \n",
    "        # Also save the scaler\n",
    "        save_model(scaler, '../models/scaler_regression.pkl')\n",
    "        \n",
    "        print(f\"‚úÖ Best regressor ({best_reg_model}) saved\")\n",
    "    \n",
    "    # Save best classification model\n",
    "    if 'pass_fail' in df_processed.columns:\n",
    "        if best_class_model == 'Random Forest':\n",
    "            best_classifier = rf_class_grid.best_estimator_\n",
    "        elif best_class_model == 'XGBoost':\n",
    "            best_classifier = xgb_class_grid.best_estimator_\n",
    "        else:\n",
    "            best_classifier = lr_class\n",
    "        \n",
    "        save_model(best_classifier, '../models/best_classifier.pkl')\n",
    "        \n",
    "        # Also save the scaler\n",
    "        save_model(scaler, '../models/scaler_classification.pkl')\n",
    "        \n",
    "        print(f\"‚úÖ Best classifier ({best_class_model}) saved\")\n",
    "    \n",
    "    # Save label encoders\n",
    "    if label_encoders:\n",
    "        save_model(label_encoders, '../models/label_encoders.pkl')\n",
    "        print(\"‚úÖ Label encoders saved\")\n",
    "    \n",
    "    # Save feature names\n",
    "    save_model(feature_cols, '../models/feature_names.pkl')\n",
    "    print(\"‚úÖ Feature names saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Explainability (SHAP)\n",
    "\n",
    "Use SHAP to understand model predictions and feature importance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if df is not None:\n",
    "    print(\"üîç Model Explainability with SHAP\\n\")\n",
    "    \n",
    "    # SHAP for classification model\n",
    "    if 'pass_fail' in df_processed.columns:\n",
    "        print(\"üìä SHAP Analysis for Classification Model\")\n",
    "        \n",
    "        try:\n",
    "            # Create SHAP explainer\n",
    "            if best_class_model in ['Random Forest', 'XGBoost']:\n",
    "                explainer = shap.TreeExplainer(best_classifier)\n",
    "                shap_values = explainer.shap_values(X_test_class[:100])  # Use first 100 samples\n",
    "                \n",
    "                # For binary classification, use class 1 SHAP values\n",
    "                if isinstance(shap_values, list):\n",
    "                    shap_values = shap_values[1]\n",
    "                \n",
    "            else:  # Logistic Regression\n",
    "                explainer = shap.LinearExplainer(best_classifier, X_train_class_scaled)\n",
    "                shap_values = explainer.shap_values(X_test_class_scaled[:100])\n",
    "            \n",
    "            # Summary plot\n",
    "            plt.figure(figsize=(10, 6))\n",
    "            shap.summary_plot(shap_values, X_test_class[:100], feature_names=feature_cols, show=False)\n",
    "            plt.tight_layout()\n",
    "            plt.savefig('../reports/figures/shap_summary_classification.png', dpi=300, bbox_inches='tight')\n",
    "            plt.show()\n",
    "            \n",
    "            # Feature importance plot\n",
    "            plt.figure(figsize=(10, 6))\n",
    "            shap.summary_plot(shap_values, X_test_class[:100], feature_names=feature_cols, plot_type=\"bar\", show=False)\n",
    "            plt.tight_layout()\n",
    "            plt.savefig('../reports/figures/shap_importance_classification.png', dpi=300, bbox_inches='tight')\n",
    "            plt.show()\n",
    "            \n",
    "            print(\"‚úÖ SHAP plots saved for classification model\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è  SHAP analysis failed: {str(e)}\")\n",
    "            print(\"Showing feature importance from tree model instead:\")\n",
    "            \n",
    "            if hasattr(best_classifier, 'feature_importances_'):\n",
    "                importance_df = pd.DataFrame({\n",
    "                    'feature': feature_cols,\n",
    "                    'importance': best_classifier.feature_importances_\n",
    "                }).sort_values('importance', ascending=False)\n",
    "                \n",
    "                plt.figure(figsize=(10, 6))\n",
    "                sns.barplot(data=importance_df.head(10), x='importance', y='feature')\n",
    "                plt.title('Top 10 Feature Importances')\n",
    "                plt.xlabel('Importance')\n",
    "                plt.tight_layout()\n",
    "                plt.savefig('../reports/figures/feature_importance_classification.png', dpi=300, bbox_inches='tight')\n",
    "                plt.show()\n",
    "    \n",
    "    # SHAP for regression model (if different from classification)\n",
    "    if 'final_marks' in df_processed.columns and best_reg_model != best_class_model:\n",
    "        print(\"\\nüìà SHAP Analysis for Regression Model\")\n",
    "        \n",
    "        try:\n",
    "            if best_reg_model in ['Random Forest', 'XGBoost']:\n",
    "                explainer_reg = shap.TreeExplainer(best_regressor)\n",
    "                shap_values_reg = explainer_reg.shap_values(X_test_reg[:100])\n",
    "            else:\n",
    "                explainer_reg = shap.LinearExplainer(best_regressor, X_train_reg_scaled)\n",
    "                shap_values_reg = explainer_reg.shap_values(X_test_reg_scaled[:100])\n",
    "            \n",
    "            plt.figure(figsize=(10, 6))\n",
    "            shap.summary_plot(shap_values_reg, X_test_reg[:100], feature_names=feature_cols, show=False)\n",
    "            plt.tight_layout()\n",
    "            plt.savefig('../reports/figures/shap_summary_regression.png', dpi=300, bbox_inches='tight')\n",
    "            plt.show()\n",
    "            \n",
    "            print(\"‚úÖ SHAP plots saved for regression model\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è  SHAP analysis failed for regression: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Quick Inference Example\n",
    "\n",
    "Demonstrate how to load and use the trained models for predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if df is not None:\n",
    "    print(\"üîÆ Model Inference Example\\n\")\n",
    "    \n",
    "    # Load saved models\n",
    "    print(\"üì• Loading saved models...\")\n",
    "    \n",
    "    if 'pass_fail' in df_processed.columns:\n",
    "        loaded_classifier = load_model('../models/best_classifier.pkl')\n",
    "        loaded_scaler = load_model('../models/scaler_classification.pkl')\n",
    "        loaded_encoders = load_model('../models/label_encoders.pkl')\n",
    "        loaded_features = load_model('../models/feature_names.pkl')\n",
    "        \n",
    "        if all([loaded_classifier, loaded_scaler, loaded_features]):\n",
    "            print(\"\\nüéØ Classification Inference Example:\")\n",
    "            \n",
    "            # Take a sample from test set\n",
    "            sample_idx = 0\n",
    "            sample_data = X_test_class.iloc[sample_idx:sample_idx+1]\n",
    "            actual_label = y_test_class.iloc[sample_idx]\n",
    "            \n",
    "            print(f\"Sample student data:\")\n",
    "            for col, val in sample_data.iloc[0].items():\n",
    "                print(f\"  {col}: {val}\")\n",
    "            \n",
    "            # Make prediction\n",
    "            sample_scaled = loaded_scaler.transform(sample_data)\n",
    "            prediction = loaded_classifier.predict(sample_scaled)[0]\n",
    "            prediction_proba = loaded_classifier.predict_proba(sample_scaled)[0]\n",
    "            \n",
    "            print(f\"\\nüìä Prediction Results:\")\n",
    "            print(f\"  Actual: {actual_label}\")\n",
    "            print(f\"  Predicted: {prediction}\")\n",
    "            print(f\"  Confidence: {max(prediction_proba):.3f}\")\n",
    "            print(f\"  Probabilities: Fail={prediction_proba[0]:.3f}, Pass={prediction_proba[1]:.3f}\")\n",
    "            \n",
    "            # Risk assessment\n",
    "            risk_score = 1 - prediction_proba[1]  # Higher risk = lower pass probability\n",
    "            if risk_score > 0.7:\n",
    "                risk_level = \"üî¥ HIGH RISK\"\n",
    "            elif risk_score > 0.4:\n",
    "                risk_level = \"üü° MEDIUM RISK\"\n",
    "            else:\n",
    "                risk_level = \"üü¢ LOW RISK\"\n",
    "            \n",
    "            print(f\"\\n‚ö†Ô∏è  Risk Assessment: {risk_level} (Score: {risk_score:.3f})\")\n",
    "    \n",
    "    # Regression inference example\n",
    "    if 'final_marks' in df_processed.columns:\n",
    "        loaded_regressor = load_model('../models/best_regressor.pkl')\n",
    "        \n",
    "        if loaded_regressor:\n",
    "            print(\"\\nüìà Regression Inference Example:\")\n",
    "            \n",
    "            sample_data_reg = X_test_reg.iloc[0:1]\n",
    "            actual_marks = y_test_reg.iloc[0]\n",
    "            \n",
    "            # Make prediction\n",
    "            if best_reg_model == 'Linear Regression':\n",
    "                sample_scaled_reg = scaler.transform(sample_data_reg)\n",
    "                predicted_marks = loaded_regressor.predict(sample_scaled_reg)[0]\n",
    "            else:\n",
    "                predicted_marks = loaded_regressor.predict(sample_data_reg)[0]\n",
    "            \n",
    "            print(f\"  Actual marks: {actual_marks:.1f}\")\n",
    "            print(f\"  Predicted marks: {predicted_marks:.1f}\")\n",
    "            print(f\"  Prediction error: {abs(actual_marks - predicted_marks):.1f}\")\n",
    "    \n",
    "    print(\"\\n‚úÖ Inference examples completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Conclusions & Next Steps\n",
    "\n",
    "### Summary of Results\n",
    "\n",
    "This notebook has successfully implemented a complete machine learning pipeline for student success prediction:\n",
    "\n",
    "**Key Achievements:**\n",
    "- ‚úÖ Data preprocessing and feature engineering\n",
    "- ‚úÖ Multiple model training and comparison\n",
    "- ‚úÖ Hyperparameter optimization\n",
    "- ‚úÖ Model evaluation and selection\n",
    "- ‚úÖ Model explainability with SHAP\n",
    "- ‚úÖ Model persistence for deployment\n",
    "\n",
    "**Best Models:**\n",
    "- **Classification:** Identifies at-risk students for early intervention\n",
    "- **Regression:** Predicts final marks for academic planning\n",
    "\n",
    "### Next Steps for Production Deployment\n",
    "\n",
    "1. **Django Integration:**\n",
    "   - Create Django views to load models and make predictions\n",
    "   - Build REST API endpoints for real-time predictions\n",
    "   - Implement batch prediction for multiple students\n",
    "\n",
    "2. **Alert System:**\n",
    "   - Set up automated alerts for high-risk students\n",
    "   - Create dashboard for educators and administrators\n",
    "   - Implement email/SMS notifications\n",
    "\n",
    "3. **Enhanced Features:**\n",
    "   - Add more sophisticated feature engineering\n",
    "   - Implement time-series analysis for trend detection\n",
    "   - Include external factors (socioeconomic, health, etc.)\n",
    "\n",
    "4. **Model Monitoring:**\n",
    "   - Set up model performance monitoring\n",
    "   - Implement automated retraining pipelines\n",
    "   - Track prediction accuracy over time\n",
    "\n",
    "5. **Integration:**\n",
    "   - Connect with Student Information Systems (SIS)\n",
    "   - Integrate with Learning Management Systems (LMS)\n",
    "   - Add real-time data feeds\n",
    "\n",
    "### Files Generated\n",
    "\n",
    "- **Models:** `models/best_classifier.pkl`, `models/best_regressor.pkl`\n",
    "- **Preprocessors:** `models/scaler_*.pkl`, `models/label_encoders.pkl`\n",
    "- **Evaluations:** `reports/figures/model_comparison.csv`\n",
    "- **Visualizations:** Various plots in `reports/figures/`\n",
    "\n",
    "The trained models are now ready for integration into the SSAES web application!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}